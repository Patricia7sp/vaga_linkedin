# Projeto: Coleta e Processamento de Vagas do LinkedIn - Ãrea de Dados

## ğŸ¯ Objetivo Geral

Construir um pipeline automatizado e escalÃ¡vel para coletar, armazenar, transformar e visualizar vagas da Ã¡rea de dados (Engenharia de Dados, AnÃ¡lise de Dados, etc.) publicadas no LinkedIn. O pipeline serÃ¡ desenvolvido com foco em aprendizado prÃ¡tico e uso de tecnologias modernas de dados e IA.

---

## ğŸ§© Tecnologias Envolvidas

- **PySpark** â€“ Processamento de dados em larga escala.
- **Apache Kafka** â€“ IngestÃ£o de dados em streaming.
- **Google Cloud Storage (GCP)** â€“ Armazenamento de dados brutos e tratados.
- **Databricks (via CLI)** â€“ ManipulaÃ§Ã£o, transformaÃ§Ã£o e anÃ¡lise dos dados.
- **Terraform** â€“ Provisionamento da infraestrutura como cÃ³digo.
- **Agentes de IA** â€“ Multi-agentes com responsabilidades especÃ­ficas.
- **LinkedIn** â€“ Fonte de dados (vagas).
- **Dashboards (Databricks ou externos)** â€“ VisualizaÃ§Ã£o dos insights.

---

## ğŸ§  Arquitetura de Agentes IA

O projeto serÃ¡ assistido por agentes de IA especializados, cada um com sua funÃ§Ã£o:

- `infra_agent`: Provisiona a infraestrutura com Terraform.
- `extract_agent`: Extrai as vagas do LinkedIn e envia ao Kafka.
- `load_agent`: Carrega dados do Storage para o Databricks.
- `transform_agent`: Processa e modela os dados no Databricks.
- `viz_agent`: Gera dashboards com base nos dados tratados.
- `control_agent`: Orquestra todas as etapas e validaÃ§Ãµes do projeto.

---

## ğŸ§± Etapas do Projeto

### âœ… Etapa 1: Infraestrutura
- **Tecnologias:** Terraform, GCP, Databricks
- **Objetivos:**
  - Criar buckets no Cloud Storage.
  - Criar o workspace e configs no Databricks.
  - Conceder permissÃµes entre Databricks e GCP.
- **Agente responsÃ¡vel:** `infra_agent`

---

### âœ… Etapa 2: ExtraÃ§Ã£o de Vagas do LinkedIn (Streaming)
- **Tecnologias:** Kafka, PySpark, LinkedIn
- **Objetivos:**
  - Configurar Kafka para receber dados em tempo real.
  - Criar script PySpark Structured Streaming.
- **Agente responsÃ¡vel:** `extract_agent`

---

### âœ… Etapa 3: Armazenamento no Cloud Storage
- **Tecnologias:** GCP Cloud Storage
- **Objetivos:**
  - Gravar os dados extraÃ­dos em JSON ou Parquet.
  - Organizar os dados por data e tipo de vaga.
- **Agente responsÃ¡vel:** `extract_agent`

---

### âœ… Etapa 4: IntegraÃ§Ã£o com Databricks via CLI
- **Tecnologias:** Databricks CLI, PySpark
- **Objetivos:**
  - Conectar o Storage ao Databricks.
  - Carregar os dados para anÃ¡lise.
- **Agente responsÃ¡vel:** `load_agent`

---

### âœ… Etapa 5: TransformaÃ§Ã£o dos Dados
- **Tecnologias:** PySpark no Databricks
- **Objetivos:**
  - Limpeza e modelagem dos dados.
  - CriaÃ§Ã£o de tabelas organizadas (vaga, empresa, localizaÃ§Ã£o etc.).
- **Agente responsÃ¡vel:** `transform_agent`

---

### âœ… Etapa 6: VisualizaÃ§Ã£o dos Dados
- **Tecnologias:** Dashboards do Databricks ou externos (Power BI, Looker etc.)
- **Objetivos:**
  - Visualizar principais mÃ©tricas: quantidade de vagas, empresas, tecnologias exigidas etc.
- **Agente responsÃ¡vel:** `viz_agent`

---

### âœ… Etapa 7: OrquestraÃ§Ã£o e ValidaÃ§Ã£o com Agentes IA
- **Tecnologias:** API + Prompt Engineering + Agentes IA
- **Objetivos:**
  - Cada etapa serÃ¡ aprovada manualmente pelo usuÃ¡rio antes de continuar.
  - O `control_agent` garantirÃ¡ a sequÃªncia correta entre as etapas.

---

## ğŸ“Œ Forma de ExecuÃ§Ã£o

1. Cada etapa serÃ¡ ativada manualmente pelo usuÃ¡rio.
2. ApÃ³s validaÃ§Ã£o, o agente `control_agent` libera o avanÃ§o para a prÃ³xima fase.
3. O projeto serÃ¡ iterativo e orientado por feedback incremental.

---

## ğŸš§ Status Inicial

- [ ] Etapa 1: Infraestrutura â€“ **em preparaÃ§Ã£o**
- [ ] Etapa 2: ExtraÃ§Ã£o Streaming â€“ **aguardando infraestrutura**
- [ ] Etapa 3: Armazenamento â€“ **aguardando dados**
- [ ] Etapa 4: IntegraÃ§Ã£o com Databricks â€“ **pendente**
- [ ] Etapa 5: TransformaÃ§Ã£o â€“ **pendente**
- [ ] Etapa 6: VisualizaÃ§Ã£o â€“ **em planejamento**
- [ ] Etapa 7: OrquestraÃ§Ã£o com IA â€“ **em desenvolvimento**

---

## ğŸ“ ObservaÃ§Ãµes

- O projeto Ã© de cunho pessoal e visa aprendizado com tecnologias de dados em ambiente realista.
- A plataforma LinkedIn serÃ¡ utilizada como fonte de dados respeitando seus termos de uso.

---

