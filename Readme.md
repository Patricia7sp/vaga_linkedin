# Projeto: Coleta e Processamento de Vagas do LinkedIn - Ãrea de Dados

## ğŸ¯ Objetivo Geral

Construir um pipeline automatizado e escalÃ¡vel para coletar, armazenar, transformar e visualizar vagas da Ã¡rea de dados (Engenharia de Dados, AnÃ¡lise de Dados, etc.) publicadas no LinkedIn. O pipeline serÃ¡ desenvolvido com foco em aprendizado prÃ¡tico e uso de tecnologias modernas de dados e IA.

---

## ğŸ§© Tecnologias Envolvidas

- **PySpark** â€“ Processamento de dados em larga escala.
- **Apache Kafka** â€“ IngestÃ£o de dados em streaming.
- **Google Cloud Storage (GCP)** â€“ Armazenamento de dados brutos e tratados.
- **Databricks (via CLI)** â€“ ManipulaÃ§Ã£o, transformaÃ§Ã£o e anÃ¡lise dos dados.
- **Terraform** â€“ Provisionamento da infraestrutura como cÃ³digo.
- **Agentes de IA** â€“ Multi-agentes com responsabilidades especÃ­ficas.
- **LinkedIn** â€“ Fonte de dados (vagas).
- **Dashboards (Databricks ou externos)** â€“ VisualizaÃ§Ã£o dos insights.

---

## ğŸ§  Arquitetura de Agentes IA

O projeto serÃ¡ assistido por agentes de IA especializados, cada um com sua funÃ§Ã£o:

- `infra_agent`: Provisiona a infraestrutura com Terraform.
- `extract_agent`: Extrai as vagas do LinkedIn e envia ao Kafka.
- `load_agent`: Carrega dados do Storage para o Databricks.
- `transform_agent`: Processa e modela os dados no Databricks.
- `viz_agent`: Gera dashboards com base nos dados tratados.
- `control_agent`: Orquestra todas as etapas e validaÃ§Ãµes do projeto.

---

## ğŸ§± Etapas do Projeto

### âœ… Etapa 1: Infraestrutura
- **Tecnologias:** Terraform, GCP, Databricks
- **Objetivos:**
  - Criar buckets no Cloud Storage.
  - Criar o workspace e configs no Databricks.
  - Conceder permissÃµes entre Databricks e GCP.
- **Agente responsÃ¡vel:** `infra_agent`

---

### âœ… Etapa 2: ExtraÃ§Ã£o de Vagas do LinkedIn (Streaming)
- **Tecnologias:** Kafka, PySpark, LinkedIn
- **Objetivos:**
  - Configurar Kafka para receber dados em tempo real.
  - Criar script PySpark Structured Streaming.
- **Agente responsÃ¡vel:** `extract_agent`

---

### âœ… Etapa 3: Armazenamento no Cloud Storage
- **Tecnologias:** GCP Cloud Storage
- **Objetivos:**
  - Gravar os dados extraÃ­dos em JSON ou Parquet.
  - Organizar os dados por data e tipo de vaga.
- **Agente responsÃ¡vel:** `extract_agent`

---

### âœ… Etapa 4: IntegraÃ§Ã£o com Databricks via CLI
- **Tecnologias:** Databricks CLI, PySpark
- **Objetivos:**
  - Conectar o Storage ao Databricks.
  - Carregar os dados para anÃ¡lise.
- **Agente responsÃ¡vel:** `load_agent`

---

### âœ… Etapa 5: TransformaÃ§Ã£o dos Dados
- **Tecnologias:** PySpark no Databricks
- **Objetivos:**
  - Limpeza e modelagem dos dados.
  - CriaÃ§Ã£o de tabelas organizadas (vaga, empresa, localizaÃ§Ã£o etc.).
- **Agente responsÃ¡vel:** `transform_agent`

---

### âœ… Etapa 6: VisualizaÃ§Ã£o dos Dados
- **Tecnologias:** Dashboards do Databricks ou externos (Power BI, Looker etc.)
- **Objetivos:**
  - Visualizar principais mÃ©tricas: quantidade de vagas, empresas, tecnologias exigidas etc.
- **Agente responsÃ¡vel:** `viz_agent`

---

### âœ… Etapa 7: OrquestraÃ§Ã£o e ValidaÃ§Ã£o com Agentes IA
- **Tecnologias:** API + Prompt Engineering + Agentes IA
- **Objetivos:**
  - Cada etapa serÃ¡ aprovada manualmente pelo usuÃ¡rio antes de continuar.
  - O `control_agent` garantirÃ¡ a sequÃªncia correta entre as etapas.

---

## ğŸ“Œ Forma de ExecuÃ§Ã£o

1. Cada etapa serÃ¡ ativada manualmente pelo usuÃ¡rio.
2. ApÃ³s validaÃ§Ã£o, o agente `control_agent` libera o avanÃ§o para a prÃ³xima fase.
3. O projeto serÃ¡ iterativo e orientado por feedback incremental.

---

## ğŸ† Conquistas e Resultados TÃ©cnicos

### âœ… **Pipeline de Dados End-to-End Funcional**
Desenvolvido e implementado com sucesso um sistema completo de extraÃ§Ã£o, processamento e armazenamento de dados em tempo real, demonstrando competÃªncias avanÃ§adas em:

- **ğŸ—ï¸ Arquitetura de Dados Moderna:** Infraestrutura cloud-native no Google Cloud Platform com Service Accounts e IAM configurados
- **âš¡ Streaming de Dados em Tempo Real:** Apache Kafka + Zookeeper + PySpark Structured Streaming operacional
- **â˜ï¸ IntegraÃ§Ã£o Cloud:** SincronizaÃ§Ã£o automÃ¡tica com Google Cloud Storage, dados organizados e versionados
- **ğŸ¤– Engenharia de IA:** Sistema multi-agentes implementado para orquestraÃ§Ã£o inteligente do pipeline

### ğŸ¯ **CompetÃªncias TÃ©cnicas Demonstradas**
- **Engenharia de Dados:** PySpark, Apache Kafka, Data Streaming, ETL/ELT
- **Cloud Computing:** Google Cloud Platform, Storage, IAM, Service Accounts  
- **DevOps & Infraestrutura:** Terraform, Docker, AutomaÃ§Ã£o de Deploy
- **InteligÃªncia Artificial:** Agentes autÃ´nomos, Prompt Engineering, AutomaÃ§Ã£o Inteligente
- **Desenvolvimento:** Python, Selenium, APIs, MicroserviÃ§os

### ğŸš€ **PrÃ³ximos Marcos**
- IntegraÃ§Ã£o com Databricks para anÃ¡lises avanÃ§adas
- Dashboards e visualizaÃ§Ãµes de dados interativas
- ExpansÃ£o do sistema para outras fontes de dados

---

## âš™ï¸ ConfiguraÃ§Ã£o e PrÃ©-requisitos

### DependÃªncias NecessÃ¡rias
- Python 3.8+
- Apache Kafka e Zookeeper
- Google Cloud SDK (gcloud CLI)
- Java 8+ (para Kafka/Spark)
- Selenium WebDriver

### ConfiguraÃ§Ã£o do Ambiente

1. **Instalar dependÃªncias:**
   ```bash
   pip install -r requirements.txt
   brew install kafka zookeeper  # macOS
   ```

2. **Configurar serviÃ§os:**
   ```bash
   brew services start zookeeper
   brew services start kafka
   ```

3. **Configurar credenciais GCP:**
   ```bash
   # Criar Service Account no GCP Console
   gcloud iam service-accounts create linkedin-scraper
   gcloud iam service-accounts keys create gcp-credentials.json --iam-account=linkedin-scraper@[PROJECT-ID].iam.gserviceaccount.com
   ```

4. **Configurar arquivo .env:**
   ```env
   GOOGLE_APPLICATION_CREDENTIALS=./gcp-credentials.json
   GCP_BUCKET_NAME=linkedin-dados-raw
   GCP_PROJECT_ID=vaga-linkedin
   KAFKA_BOOTSTRAP_SERVERS=localhost:9092
   KAFKA_TOPIC=vagas_dados
   ```

---

## ğŸš€ Como Executar

### ExecuÃ§Ã£o Completa (Modo Streaming + GCP)
```bash
python main.py
```

### ExecuÃ§Ã£o Apenas ExtraÃ§Ã£o (Modo Offline)
```bash
python run_extraction_only.py
```

### Verificar Status dos ServiÃ§os
```bash
# Verificar Kafka/Zookeeper
brew services list | grep -E "(kafka|zookeeper)"

# Verificar autenticaÃ§Ã£o GCP
gcloud auth list
export GOOGLE_APPLICATION_CREDENTIALS=./gcp-credentials.json
gcloud auth application-default print-access-token
```

---

## ğŸ“Š Resultados Obtidos

### Pipeline Funcional
- âœ… **ExtraÃ§Ã£o automatizada** de vagas do LinkedIn (6 categorias)
- âœ… **Processamento em tempo real** via Kafka/PySpark
- âœ… **Armazenamento na nuvem** (GCP Cloud Storage)
- âœ… **Dados estruturados** em formato JSONL organizados por data

### Dados Coletados
- **Categorias:** Data Engineer, Data Analytics, Digital Analytics
- **Campos extraÃ­dos:** tÃ­tulo, empresa, localizaÃ§Ã£o, descriÃ§Ã£o, skills, salÃ¡rio
- **Volume:** ~6 vagas por execuÃ§Ã£o (limitado para testes)
- **Formato:** JSONL com estrutura padronizada

### Arquitetura Implementada
```
LinkedIn â†’ Selenium â†’ Kafka â†’ PySpark â†’ GCP Storage
                              â†“
                         Processamento
                         + Enriquecimento
```

---

## ğŸ”„ Pipeline CI/CD (GitHub â†’ Cloud Run)

```
1. git push â†’ 2. GitHub Actions â†’ 3. Build Docker â†’ 4. Update Cloud Run â†’ 5. Deploy AutomÃ¡tico
```

**Workflows:**
- âœ… ci-cd-pipeline.yml - Deploy Cloud Run
- âœ… databricks-deploy.yml - Deploy Databricks  
- âœ… auto-promote-to-prod.yml - Auto-promoÃ§Ã£o

**Quality Gates:**
- Code Quality (Black, Flake8, Pylint)
- Unit Tests (pytest > 70% coverage)
- Security Scan (Bandit, Trivy)

---

## ğŸ“ ObservaÃ§Ãµes

- O projeto Ã© de cunho pessoal e visa aprendizado com tecnologias de dados em ambiente realista.
- A plataforma LinkedIn serÃ¡ utilizada como fonte de dados respeitando seus termos de uso.

---

