# Projeto: Coleta e Processamento de Vagas do LinkedIn - √Årea de Dados

## üéØ Objetivo Geral

Construir um pipeline automatizado e escal√°vel para coletar, armazenar, transformar e visualizar vagas da √°rea de dados (Engenharia de Dados, An√°lise de Dados, etc.) publicadas no LinkedIn. O pipeline ser√° desenvolvido com foco em aprendizado pr√°tico e uso de tecnologias modernas de dados e IA.

---

## üß© Tecnologias Envolvidas

- **PySpark** ‚Äì Processamento de dados em larga escala.
- **Apache Kafka** ‚Äì Ingest√£o de dados em streaming.
- **Google Cloud Storage (GCP)** ‚Äì Armazenamento de dados brutos e tratados.
- **Databricks (via CLI)** ‚Äì Manipula√ß√£o, transforma√ß√£o e an√°lise dos dados.
- **Terraform** ‚Äì Provisionamento da infraestrutura como c√≥digo.
- **Agentes de IA** ‚Äì Multi-agentes com responsabilidades espec√≠ficas.
- **LinkedIn** ‚Äì Fonte de dados (vagas).
- **Dashboards (Databricks ou externos)** ‚Äì Visualiza√ß√£o dos insights.

---

## üß† Arquitetura de Agentes IA

O projeto ser√° assistido por agentes de IA especializados, cada um com sua fun√ß√£o:

- `infra_agent`: Provisiona a infraestrutura com Terraform.
- `extract_agent`: Extrai as vagas do LinkedIn e envia ao Kafka.
- `load_agent`: Carrega dados do Storage para o Databricks.
- `transform_agent`: Processa e modela os dados no Databricks.
- `viz_agent`: Gera dashboards com base nos dados tratados.
- `control_agent`: Orquestra todas as etapas e valida√ß√µes do projeto.

---

## üß± Etapas do Projeto

### ‚úÖ Etapa 1: Infraestrutura
- **Tecnologias:** Terraform, GCP, Databricks
- **Objetivos:**
  - Criar buckets no Cloud Storage.
  - Criar o workspace e configs no Databricks.
  - Conceder permiss√µes entre Databricks e GCP.
- **Agente respons√°vel:** `infra_agent`

---

### ‚úÖ Etapa 2: Extra√ß√£o de Vagas do LinkedIn (Streaming)
- **Tecnologias:** Kafka, PySpark, LinkedIn
- **Objetivos:**
  - Configurar Kafka para receber dados em tempo real.
  - Criar script PySpark Structured Streaming.
- **Agente respons√°vel:** `extract_agent`

---

### ‚úÖ Etapa 3: Armazenamento no Cloud Storage
- **Tecnologias:** GCP Cloud Storage
- **Objetivos:**
  - Gravar os dados extra√≠dos em JSON ou Parquet.
  - Organizar os dados por data e tipo de vaga.
- **Agente respons√°vel:** `extract_agent`

---

### ‚úÖ Etapa 4: Integra√ß√£o com Databricks via CLI
- **Tecnologias:** Databricks CLI, PySpark
- **Objetivos:**
  - Conectar o Storage ao Databricks.
  - Carregar os dados para an√°lise.
- **Agente respons√°vel:** `load_agent`

---

### ‚úÖ Etapa 5: Transforma√ß√£o dos Dados
- **Tecnologias:** PySpark no Databricks
- **Objetivos:**
  - Limpeza e modelagem dos dados.
  - Cria√ß√£o de tabelas organizadas (vaga, empresa, localiza√ß√£o etc.).
- **Agente respons√°vel:** `transform_agent`

---

### ‚úÖ Etapa 6: Visualiza√ß√£o dos Dados
- **Tecnologias:** Dashboards do Databricks ou externos (Power BI, Looker etc.)
- **Objetivos:**
  - Visualizar principais m√©tricas: quantidade de vagas, empresas, tecnologias exigidas etc.
- **Agente respons√°vel:** `viz_agent`

---

### ‚úÖ Etapa 7: Orquestra√ß√£o e Valida√ß√£o com Agentes IA
- **Tecnologias:** API + Prompt Engineering + Agentes IA
- **Objetivos:**
  - Cada etapa ser√° aprovada manualmente pelo usu√°rio antes de continuar.
  - O `control_agent` garantir√° a sequ√™ncia correta entre as etapas.

---

## üìå Forma de Execu√ß√£o

1. Cada etapa ser√° ativada manualmente pelo usu√°rio.
2. Ap√≥s valida√ß√£o, o agente `control_agent` libera o avan√ßo para a pr√≥xima fase.
3. O projeto ser√° iterativo e orientado por feedback incremental.

---

## üèÜ Conquistas e Resultados T√©cnicos

### ‚úÖ **Pipeline de Dados End-to-End Funcional**
Desenvolvido e implementado com sucesso um sistema completo de extra√ß√£o, processamento e armazenamento de dados em tempo real, demonstrando compet√™ncias avan√ßadas em:

- **üèóÔ∏è Arquitetura de Dados Moderna:** Infraestrutura cloud-native no Google Cloud Platform com Service Accounts e IAM configurados
- **‚ö° Streaming de Dados em Tempo Real:** Apache Kafka + Zookeeper + PySpark Structured Streaming operacional
- **‚òÅÔ∏è Integra√ß√£o Cloud:** Sincroniza√ß√£o autom√°tica com Google Cloud Storage, dados organizados e versionados
- **ü§ñ Engenharia de IA:** Sistema multi-agentes implementado para orquestra√ß√£o inteligente do pipeline

### üéØ **Compet√™ncias T√©cnicas Demonstradas**
- **Engenharia de Dados:** PySpark, Apache Kafka, Data Streaming, ETL/ELT
- **Cloud Computing:** Google Cloud Platform, Storage, IAM, Service Accounts  
- **DevOps & Infraestrutura:** Terraform, Docker, Automa√ß√£o de Deploy
- **Intelig√™ncia Artificial:** Agentes aut√¥nomos, Prompt Engineering, Automa√ß√£o Inteligente
- **Desenvolvimento:** Python, Selenium, APIs, Microservi√ßos

### üöÄ **Pr√≥ximos Marcos**
- Integra√ß√£o com Databricks para an√°lises avan√ßadas
- Dashboards e visualiza√ß√µes de dados interativas
- Expans√£o do sistema para outras fontes de dados

---

## ‚öôÔ∏è Configura√ß√£o e Pr√©-requisitos

### Depend√™ncias Necess√°rias
- Python 3.8+
- Apache Kafka e Zookeeper
- Google Cloud SDK (gcloud CLI)
- Java 8+ (para Kafka/Spark)
- Selenium WebDriver

### Configura√ß√£o do Ambiente

1. **Instalar depend√™ncias:**
   ```bash
   pip install -r requirements.txt
   brew install kafka zookeeper  # macOS
   ```

2. **Configurar servi√ßos:**
   ```bash
   brew services start zookeeper
   brew services start kafka
   ```

3. **Configurar credenciais GCP:**
   ```bash
   # Criar Service Account no GCP Console
   gcloud iam service-accounts create linkedin-scraper
   gcloud iam service-accounts keys create gcp-credentials.json --iam-account=linkedin-scraper@[PROJECT-ID].iam.gserviceaccount.com
   ```

4. **Configurar arquivo .env:**
   ```env
   GOOGLE_APPLICATION_CREDENTIALS=./gcp-credentials.json
   GCP_BUCKET_NAME=linkedin-dados-raw
   GCP_PROJECT_ID=vaga-linkedin
   KAFKA_BOOTSTRAP_SERVERS=localhost:9092
   KAFKA_TOPIC=vagas_dados
   ```

---

## üöÄ Como Executar

### Execu√ß√£o Completa (Modo Streaming + GCP)
```bash
python main.py
```

### Execu√ß√£o Apenas Extra√ß√£o (Modo Offline)
```bash
python run_extraction_only.py
```

### Verificar Status dos Servi√ßos
```bash
# Verificar Kafka/Zookeeper
brew services list | grep -E "(kafka|zookeeper)"

# Verificar autentica√ß√£o GCP
gcloud auth list
export GOOGLE_APPLICATION_CREDENTIALS=./gcp-credentials.json
gcloud auth application-default print-access-token
```

---

## üìä Resultados Obtidos

### Pipeline Funcional
- ‚úÖ **Extra√ß√£o automatizada** de vagas do LinkedIn (6 categorias)
- ‚úÖ **Processamento em tempo real** via Kafka/PySpark
- ‚úÖ **Armazenamento na nuvem** (GCP Cloud Storage)
- ‚úÖ **Dados estruturados** em formato JSONL organizados por data

### Dados Coletados
- **Categorias:** Data Engineer, Data Analytics, Digital Analytics
- **Campos extra√≠dos:** t√≠tulo, empresa, localiza√ß√£o, descri√ß√£o, skills, sal√°rio
- **Volume:** ~6 vagas por execu√ß√£o (limitado para testes)
- **Formato:** JSONL com estrutura padronizada

### Arquitetura Implementada
```
LinkedIn ‚Üí Selenium ‚Üí Kafka ‚Üí PySpark ‚Üí GCP Storage
                              ‚Üì
                         Processamento
                         + Enriquecimento
```

---

## üìé Observa√ß√µes

- O projeto √© de cunho pessoal e visa aprendizado com tecnologias de dados em ambiente realista.
- A plataforma LinkedIn ser√° utilizada como fonte de dados respeitando seus termos de uso.

---

