# ğŸ§  Etapa 2 â€“ extract_agent com Kafka + PySpark Streaming

Este agente Ã© responsÃ¡vel por capturar dados de vagas do LinkedIn em tempo real, utilizando Apache Kafka como mecanismo de ingestÃ£o e PySpark Structured Streaming como motor de processamento. Os dados serÃ£o tratados e enviados diretamente para o bucket GCP em arquivos JSON.


## Kafka
Broker local ou remoto (pode usar Docker ou serviÃ§o gerenciado).
Criar tÃ³pico Ãºnico: vagas_dados.
Mensagens no tÃ³pico devem ser formatadas como JSON com chave "type".

## PySpark
Job PySpark deve:
Ler do tÃ³pico Kafka em tempo real.
Filtrar os dados por tipo de vaga.
Escrever em JSON para o caminho no bucket correspondente

## exemplo de script pyspark
```

query = df \
  .filter("type = 'data_engineer'") \
  .writeStream \
  .format("json") \
  .option("path", "gs://linkedin-dados-raw/data_engineer/") \
  .option("checkpointLocation", "/tmp/checkpoints/data_engineer") \
  .start()
```


---

## ğŸ¯ Objetivo Geral

- Conectar Ã  API do LinkedIn utilizando autenticaÃ§Ã£o via token.
- Realizar a extraÃ§Ã£o das vagas com base em trÃªs filtros especÃ­ficos.
- Salvar os dados extraÃ­dos em arquivos separados e organizados no bucket GCP.

--

## Tecnologias Utilizadas
- PySpark Structured Streaming
- Apache Kafka
- LinkedIn API ou Web Scraping
- Cloud Storage (GCP)-

## ğŸ“š Bibliotecas Utilizadas
- requests
- json
- os
- subprocess
- time
- datetime
- dotenv    



## ğŸ” Filtros de Busca

A busca deverÃ¡ ser segmentada por trÃªs perfis distintos de vaga:

1. **Data Engineer** â€“ Vagas com tÃ­tulos como â€œEngenheiro de Dadosâ€ ou â€œData Engineerâ€.
2. **Data Analytics** â€“ Vagas com tÃ­tulos como â€œAnalista de Dadosâ€ ou â€œData Analyticsâ€.
3. **Digital Analytics** â€“ Vagas relacionadas a mÃ©tricas digitais, produtos, marketing, etc.

Cada tipo de vaga gerarÃ¡ um **arquivo JSON independente**, nomeado da seguinte forma:

- `data_engineer_<data>.json`
- `data_analytics_<data>.json`
- `digital_analytics_<data>.json`

Exemplo de caminho no bucket:
- gs://linkedin-dados-raw/data_engineer/2025-09-01/data_engineer_2025-09-01.json

## ğŸ” AutenticaÃ§Ã£o na API do LinkedIn

O acesso Ã  API oficial do LinkedIn exige:

- Conta de desenvolvedor (via [LinkedIn Developer Portal](https://www.linkedin.com/developers))
- CriaÃ§Ã£o de uma aplicaÃ§Ã£o
- GeraÃ§Ã£o de um **token OAuth 2.0** com os escopos necessÃ¡rios (ex: `r_jobs`, `r_ads`, etc.)

### ğŸš¨ Ponto de atenÃ§Ã£o:
Caso a API oficial nÃ£o permita esse tipo de consulta, serÃ¡ necessÃ¡rio utilizar **web scraping responsÃ¡vel** (com limitaÃ§Ãµes de taxa e respeitando os termos de uso).



---

## ğŸ§  Responsabilidades do `extract_agent`

1. **Autenticar-se** na API do LinkedIn com o token OAuth.
2. Realizar **trÃªs requisiÃ§Ãµes separadas**, com base nos filtros definidos.
3. Tratar e organizar os dados brutos: cargo, empresa, localizaÃ§Ã£o, data da vaga, link, etc.
4. **Salvar os dados localmente** em JSON.
5. Realizar o **upload dos arquivos para os buckets correspondentes**.
6. Retornar um **resumo estruturado** com:
   - Quantidade de vagas por tipo
   - Caminho dos arquivos no bucket
   - Logs de execuÃ§Ã£o
   - Erros (caso ocorram)

---

## âœ… ValidaÃ§Ãµes Esperadas

- Os trÃªs arquivos foram criados corretamente?
- Foram salvos no bucket correto e com data correspondente?
- Houve sucesso na autenticaÃ§Ã£o com a API do LinkedIn?
- A estrutura dos dados contÃ©m os seguintes campos:
  - `job_title`, `company_name`, `location`, `posted_date`, `job_url`, `description_snippet`

---

## ğŸ“ Pontos de AtenÃ§Ã£o

- A API do LinkedIn pode limitar o nÃºmero de requisiÃ§Ãµes por hora.
- Os tokens OAuth expiram â€” o agente deve verificar e renovar se necessÃ¡rio.
- Se scraping for usado como fallback, respeitar o `robots.txt` e limitar o nÃºmero de requisiÃ§Ãµes (delay de 2â€“5 segundos entre requisiÃ§Ãµes).
- Armazenar os arquivos com **timestamp diÃ¡rio** para controle histÃ³rico.
- Verificar se os arquivos nÃ£o estÃ£o vazios antes de subir.

---

## ğŸ§ª Logs e SaÃ­da Esperada

Formato JSON de resposta:

```json
{
  "kafka_status": "OK",
  "spark_streaming_status": "RUNNING",
  "outputs": {
    "data_engineer": {
      "count": 102,
      "path": "gs://linkedin-dados-raw/data_engineer/..."
    },
    "data_analytics": {
      "count": 86,
      "path": "gs://linkedin-dados-raw/data_analytics/..."
    },
    "digital_analytics": {
      "count": 47,
      "path": "gs://linkedin-dados-raw/digital_analytics/..."
    }
  }
}

``` 

## ğŸ“¤ SaÃ­da Esperada

- JSON com os nomes e status dos recursos criados.
- Logs de execuÃ§Ã£o do Terraform.    



 
## Responsabilidades Atualizadas do extract_agent

Iniciar e manter o producer Kafka alimentando o tÃ³pico vagas_dados.
Validar a autenticaÃ§Ã£o com a API do LinkedIn (ou fallback scraping).
Rodar o PySpark Structured Streaming job que:
Consome o tÃ³pico vagas_dados
Processa as mensagens em tempo real
Salva os dados separados por tipo em buckets no GCP
Garantir consistÃªncia do schema e salvar em formato JSON.
Retornar relatÃ³rio de execuÃ§Ã£o com:
Contagem de mensagens por tipo
Caminhos de saÃ­da
Status dos jobs


## ğŸ“š Pontos de AtenÃ§Ã£o ReforÃ§ados
- O PySpark Structured Streaming deve ser tolerante a falhas (uso de checkpoint).
- O Kafka deve ter polÃ­tica de retenÃ§Ã£o compatÃ­vel com frequÃªncia de consumo. 

## âœ… Checklist Final da Etapa

- Kafka Broker iniciado e tÃ³pico vagas_dados criado
- Producer estÃ¡ enviando mensagens formatadas corretamente
- PySpark Structured Streaming estÃ¡ rodando e processando sem erro
- Arquivos estÃ£o sendo salvos no bucket GCP com nomes e partiÃ§Ãµes corretas
- Logs e status do job retornados ao control_agent