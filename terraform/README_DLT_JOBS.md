# ğŸš€ Terraform - DLT Jobs Automation

## ğŸ“Œ O que este mÃ³dulo faz?

Cria **automaticamente** os 3 jobs do Databricks para executar os pipelines DLT:

1. âœ… `dlt-data-engineer-pipeline`
2. âœ… `dlt-data-analytics-pipeline`
3. âœ… `dlt-digital-analytics-pipeline`

**Vantagens:**
- âœ… **Infraestrutura como CÃ³digo** (versionada no Git)
- âœ… **Sem criaÃ§Ã£o manual** via UI
- âœ… **Consistente e reproduzÃ­vel**
- âœ… **FÃ¡cil gerenciar** (update, destroy)

---

## ğŸ“‹ PrÃ©-requisitos

### 1. Terraform instalado
```bash
# Verificar instalaÃ§Ã£o
terraform version

# Se nÃ£o instalado (Mac):
brew install terraform

# Ou baixar: https://www.terraform.io/downloads
```

### 2. Credenciais Databricks
```bash
# Obter token: Databricks UI â†’ Settings â†’ Developer â†’ Access Tokens
export DATABRICKS_HOST="https://dbc-14d16b60-2882.cloud.databricks.com"
export DATABRICKS_TOKEN="dapi..."
```

---

## ğŸš€ Como usar

### **Passo 1: Configurar ambiente**

```bash
cd /usr/local/anaconda3/vaga_linkedin/terraform/

# Exportar credenciais
export DATABRICKS_HOST="https://dbc-14d16b60-2882.cloud.databricks.com"
export DATABRICKS_TOKEN="seu-token-aqui"
```

### **Passo 2: Inicializar Terraform**

```bash
terraform init
```

**Output esperado:**
```
Initializing the backend...
Initializing provider plugins...
- Finding databricks/databricks versions matching "~> 1.29"...
- Installing databricks/databricks v1.29.0...

Terraform has been successfully initialized!
```

### **Passo 3: Planejar mudanÃ§as (preview)**

```bash
terraform plan -target=databricks_job.dlt_pipeline_jobs
```

**Output esperado:**
```
Terraform will perform the following actions:

  # databricks_job.dlt_pipeline_jobs["data_engineer"] will be created
  + resource "databricks_job" "dlt_pipeline_jobs" {
      + name     = "dlt-data-engineer-pipeline"
      + schedule = {
          + quartz_cron_expression = "0 30 8,12,21 ? * MON-FRI"
          + timezone_id            = "America/Sao_Paulo"
        }
      ...
    }

  # databricks_job.dlt_pipeline_jobs["data_analytics"] will be created
  ...

  # databricks_job.dlt_pipeline_jobs["digital_analytics"] will be created
  ...

Plan: 3 to add, 0 to change, 0 to destroy.
```

### **Passo 4: Aplicar (criar jobs)**

```bash
terraform apply -target=databricks_job.dlt_pipeline_jobs
```

**Confirmar:**
```
Do you want to perform these actions?
  Enter a value: yes
```

**Output esperado:**
```
databricks_job.dlt_pipeline_jobs["data_engineer"]: Creating...
databricks_job.dlt_pipeline_jobs["data_analytics"]: Creating...
databricks_job.dlt_pipeline_jobs["digital_analytics"]: Creating...

databricks_job.dlt_pipeline_jobs["data_engineer"]: Creation complete after 2s [id=123456789]
databricks_job.dlt_pipeline_jobs["data_analytics"]: Creation complete after 2s [id=234567890]
databricks_job.dlt_pipeline_jobs["digital_analytics"]: Creation complete after 2s [id=345678901]

Apply complete! Resources: 3 added, 0 changed, 0 destroyed.

Outputs:

dlt_job_ids = {
  "data_analytics" = "234567890"
  "data_engineer" = "123456789"
  "digital_analytics" = "345678901"
}

dlt_job_urls = {
  "data_analytics" = "https://dbc-14d16b60-2882.cloud.databricks.com/#job/234567890"
  "data_engineer" = "https://dbc-14d16b60-2882.cloud.databricks.com/#job/123456789"
  "digital_analytics" = "https://dbc-14d16b60-2882.cloud.databricks.com/#job/345678901"
}
```

### **Passo 5: Validar no Databricks UI**

1. Acessar: https://dbc-14d16b60-2882.cloud.databricks.com/
2. Menu â†’ **Workflows**
3. Verificar 3 novos jobs criados:
   - âœ… `dlt-data-engineer-pipeline`
   - âœ… `dlt-data-analytics-pipeline`
   - âœ… `dlt-digital-analytics-pipeline`

---

## ğŸ“Š Estrutura criada

```
DATABRICKS JOBS (via Terraform):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dlt-data-engineer-pipeline             â”‚
â”‚  â”œâ”€ Schedule: 08:30, 12:30, 21:30      â”‚
â”‚  â”œâ”€ Notebook: linkedin_pipeline_runner  â”‚
â”‚  â”œâ”€ ParÃ¢metro: pipeline=data_engineer   â”‚
â”‚  â””â”€ Tags: pipeline, type, domain        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dlt-data-analytics-pipeline            â”‚
â”‚  â”œâ”€ Schedule: 09:00, 13:00, 22:00      â”‚
â”‚  â”œâ”€ Notebook: linkedin_pipeline_runner  â”‚
â”‚  â”œâ”€ ParÃ¢metro: pipeline=data_analytics  â”‚
â”‚  â””â”€ Tags: pipeline, type, domain        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dlt-digital-analytics-pipeline         â”‚
â”‚  â”œâ”€ Schedule: 10:00, 14:00, 23:00      â”‚
â”‚  â”œâ”€ Notebook: linkedin_pipeline_runner  â”‚
â”‚  â”œâ”€ ParÃ¢metro: pipeline=digital_analyticsâ”‚
â”‚  â””â”€ Tags: pipeline, type, domain        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ OperaÃ§Ãµes comuns

### **Ver IDs dos jobs criados**

```bash
terraform output dlt_job_ids
```

### **Ver URLs dos jobs**

```bash
terraform output dlt_job_urls
```

### **Atualizar configuraÃ§Ã£o de 1 job**

1. Editar `terraform/databricks_dlt_jobs.tf`
2. Exemplo: Mudar horÃ¡rio do data_engineer:
   ```hcl
   data_engineer = {
     ...
     cron = "0 0 9 ? * MON-FRI"  # Novo horÃ¡rio: 09:00
   }
   ```
3. Aplicar mudanÃ§a:
   ```bash
   terraform apply -target=databricks_job.dlt_pipeline_jobs[\"data_engineer\"]
   ```

### **Pausar job via Terraform**

```hcl
schedule {
  ...
  pause_status = "PAUSED"  # UNPAUSED â†’ PAUSED
}
```

```bash
terraform apply -target=databricks_job.dlt_pipeline_jobs[\"data_engineer\"]
```

### **Deletar jobs**

```bash
# Deletar apenas jobs DLT
terraform destroy -target=databricks_job.dlt_pipeline_jobs

# Ou deletar tudo (cuidado!)
terraform destroy
```

---

## âš ï¸ Troubleshooting

### **Erro: "Organization has been cancelled or is not active yet"**

**Causa:** Daily limit exhausted (temporÃ¡rio)

**SoluÃ§Ã£o:**
1. âœ… Aguardar 24h para reset
2. âœ… Tentar novamente
3. âœ… Terraform retenta automaticamente

### **Erro: "DATABRICKS_HOST not set"**

**SoluÃ§Ã£o:**
```bash
export DATABRICKS_HOST="https://dbc-14d16b60-2882.cloud.databricks.com"
export DATABRICKS_TOKEN="dapi..."
```

### **Erro: "Invalid token"**

**SoluÃ§Ã£o:**
1. Databricks UI â†’ Settings â†’ Developer â†’ Access Tokens
2. Criar novo token
3. Exportar: `export DATABRICKS_TOKEN="novo-token"`

### **Jobs nÃ£o aparecem no UI**

**SoluÃ§Ã£o:**
```bash
# Verificar se foram criados
terraform show | grep databricks_job

# Ver state
terraform state list | grep dlt_pipeline_jobs

# Ver output
terraform output dlt_job_ids
```

---

## ğŸ¯ Vantagens vs CriaÃ§Ã£o Manual (UI)

| Aspecto | Terraform âœ… | UI Manual âŒ |
|---------|-------------|-------------|
| **Tempo** | 2 minutos (1 comando) | 15 minutos (3 jobs) |
| **Erros** | Zero (validado) | PossÃ­veis (digitaÃ§Ã£o) |
| **Versionamento** | Git (histÃ³rico) | Nenhum |
| **ReproduzÃ­vel** | 100% | Depende de quem cria |
| **Rollback** | `terraform destroy` | Manual (deletar 1 por 1) |
| **DocumentaÃ§Ã£o** | CÃ³digo Ã© doc | Precisa documentar separado |
| **CI/CD** | Integra fÃ¡cil | DifÃ­cil automatizar |
| **Auditoria** | Git log | Nenhuma |

---

## ğŸ“ Arquivos Terraform

```
terraform/
â”œâ”€â”€ databricks_dlt_jobs.tf     â† NOVO - Jobs DLT (este mÃ³dulo)
â”œâ”€â”€ databricks_dlt.tf          â† Pipelines DLT (existente)
â”œâ”€â”€ databricks_files.tf        â† Notebooks (existente)
â”œâ”€â”€ agent_chat.tf              â† Job Agent Chat (existente)
â”œâ”€â”€ main.tf                    â† Provider config
â”œâ”€â”€ variables.tf               â† VariÃ¡veis
â””â”€â”€ README_DLT_JOBS.md         â† Este guia
```

---

## ğŸ”’ SeguranÃ§a

### **NÃƒO commitar tokens no Git:**

```bash
# âŒ NUNCA fazer isso:
export DATABRICKS_TOKEN="dapi123..."
git add .
git commit -m "add token"  # âŒ ERRADO!

# âœ… SEMPRE usar variÃ¡vel de ambiente:
export DATABRICKS_TOKEN="dapi..."  # Apenas na sessÃ£o local
terraform apply
```

### **Usar GitHub Secrets para CI/CD:**

```yaml
# .github/workflows/terraform.yml
env:
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
```

---

## ğŸš€ PrÃ³ximos Passos

### **1. Testar jobs criados**

ApÃ³s criar via Terraform, testar manualmente:

```bash
# Databricks UI â†’ Workflows â†’ Selecionar job â†’ "Run now"
```

Validar:
- âœ… Duration: 9-11 minutos (nÃ£o 1 minuto!)
- âœ… Status: SUCCESS
- âœ… Logs mostram pipeline correto executado

### **2. Monitorar execuÃ§Ãµes**

```bash
# Ver Ãºltimas runs
databricks jobs list-runs --job-id <job_id> --limit 5
```

### **3. Integrar com CI/CD**

Criar workflow GitHub Actions:

```yaml
name: Terraform Deploy Jobs

on:
  push:
    branches: [main]
    paths:
      - 'terraform/databricks_dlt_jobs.tf'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: hashicorp/setup-terraform@v2
      - name: Terraform Apply
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          cd terraform
          terraform init
          terraform apply -target=databricks_job.dlt_pipeline_jobs -auto-approve
```

---

## âœ… Checklist de Deploy

Antes de aplicar Terraform:

- [ ] Credenciais configuradas (`DATABRICKS_HOST`, `DATABRICKS_TOKEN`)
- [ ] Daily limit resetado (aguardar 24h se necessÃ¡rio)
- [ ] Notebook deployado (`/Shared/linkedin_pipeline_runner_notebook`)
- [ ] Pipelines DLT existem (data_engineer, data_analytics, digital_analytics)
- [ ] `terraform init` executado
- [ ] `terraform plan` revisado
- [ ] Backup do state (`terraform.tfstate`)

ApÃ³s aplicar:

- [ ] Verificar outputs (`terraform output`)
- [ ] Validar jobs no Databricks UI
- [ ] Testar 1 job manualmente
- [ ] Verificar logs e duration
- [ ] Ativar schedule se desabilitado

---

## ğŸ“š ReferÃªncias

- [Terraform Databricks Provider](https://registry.terraform.io/providers/databricks/databricks/latest/docs)
- [Databricks Job Resource](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/job)
- [Terraform Best Practices](https://www.terraform-best-practices.com/)

---

**ğŸ‰ Terraform Ã© a forma CORRETA e PROFISSIONAL de gerenciar infraestrutura Databricks!**

**Muito mais rÃ¡pido, seguro e confiÃ¡vel que criaÃ§Ã£o manual via UI! ğŸš€**
